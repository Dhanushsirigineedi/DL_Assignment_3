{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanushsirigineedi/DL_Assignment_3/blob/main/Assignment_3_without_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyFhPWoa4A3U"
      },
      "source": [
        "# Seq2Seq Model for Dakshina Transliteration\n",
        "\n",
        "This notebook implements a sequence-to-sequence model for transliteration using the Dakshina dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAeEruRQ4A3W"
      },
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cf6X_HB4A3X",
        "outputId": "1a1095b6-bd06-44f3-e31b-c6c2b67e9266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8fi77HM4A3Y",
        "outputId": "b147f37b-f39f-4ab0-a005-13745e7dfb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import subprocess\n",
        "import sys\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "import wandb\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import types\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBoFQZcd4A3Z"
      },
      "source": [
        "## 2. Vocabulary and Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UWm_yKut4A3Z"
      },
      "outputs": [],
      "source": [
        "# Vocabulary class\n",
        "class CharVocab:\n",
        "    \"\"\"Character-level vocabulary class with special tokens support\"\"\"\n",
        "    def __init__(self, tokens=None, specials=['<pad>','<sos>','<eos>','<unk>']):\n",
        "        self.specials = specials\n",
        "        self.idx2char = list(specials) + (tokens or [])\n",
        "        self.char2idx = {ch:i for i,ch in enumerate(self.idx2char)}\n",
        "\n",
        "    @classmethod\n",
        "    def build_from_texts(cls, texts):\n",
        "        \"\"\"Build vocabulary from a list of texts\"\"\"\n",
        "        chars = sorted({c for line in texts for c in line})\n",
        "        return cls(tokens=chars)\n",
        "\n",
        "    @classmethod\n",
        "    def build_from_file(cls, file_path, src_col=0, tgt_col=1, is_csv=True):\n",
        "        \"\"\"Build vocabulary from a data file (CSV or TSV)\"\"\"\n",
        "        if is_csv:\n",
        "            df = pd.read_csv(file_path, header=None)\n",
        "            texts = df[src_col].dropna().tolist() + df[tgt_col].dropna().tolist()\n",
        "        else:\n",
        "            texts = []\n",
        "            with open(file_path, encoding='utf-8') as f:\n",
        "                for ln in f:\n",
        "                    parts = ln.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        texts.extend([parts[0], parts[1]])\n",
        "\n",
        "        return cls.build_from_texts(texts)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save vocabulary to JSON file\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.idx2char, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Load vocabulary from JSON file\"\"\"\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            idx2char = json.load(f)\n",
        "\n",
        "        inst = cls(tokens=[])\n",
        "        inst.idx2char = idx2char\n",
        "        inst.char2idx = {c:i for i,c in enumerate(idx2char)}\n",
        "        return inst\n",
        "\n",
        "    def encode(self, text, add_sos=False, add_eos=False):\n",
        "        \"\"\"Convert text to a sequence of indices\"\"\"\n",
        "        seq = []\n",
        "        if add_sos: seq.append(self.char2idx['<sos>'])\n",
        "        for c in text:\n",
        "            seq.append(self.char2idx.get(c, self.char2idx['<unk>']))\n",
        "        if add_eos: seq.append(self.char2idx['<eos>'])\n",
        "        return seq\n",
        "\n",
        "    def decode(self, idxs, strip_specials=True, join=True):\n",
        "        \"\"\"Convert a sequence of indices back to text\"\"\"\n",
        "        # Convert tensor to list if needed\n",
        "        if hasattr(idxs, 'tolist'):\n",
        "            idxs = idxs.tolist()\n",
        "\n",
        "        # Convert indices to characters\n",
        "        chars = [self.idx2char[i] for i in idxs if i < len(self.idx2char)]\n",
        "\n",
        "        # Remove special tokens if requested\n",
        "        if strip_specials:\n",
        "            chars = [c for c in chars if c not in self.specials]\n",
        "\n",
        "        # Return as string or list\n",
        "        return ''.join(chars) if join else chars\n",
        "\n",
        "    def batch_decode(self, batch_idxs, strip_specials=True):\n",
        "        \"\"\"Decode a batch of index sequences\"\"\"\n",
        "        return [self.decode(seq, strip_specials=strip_specials) for seq in batch_idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2char)\n",
        "\n",
        "    @property\n",
        "    def pad_idx(self): return self.char2idx['<pad>']\n",
        "\n",
        "    @property\n",
        "    def sos_idx(self): return self.char2idx['<sos>']\n",
        "\n",
        "    @property\n",
        "    def eos_idx(self): return self.char2idx['<eos>']\n",
        "\n",
        "    @property\n",
        "    def unk_idx(self): return self.char2idx['<unk>']\n",
        "\n",
        "    @property\n",
        "    def size(self): return len(self.idx2char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P6e68hyO4A3a"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"A dataset class for Dakshina-style transliteration data\"\"\"\n",
        "\n",
        "    def __init__(self, path, src_vocab, tgt_vocab, format='dakshina'):\n",
        "        \"\"\"Initialize the dataset\"\"\"\n",
        "        self.examples = []\n",
        "        self.format = format\n",
        "\n",
        "        if format == 'dakshina':\n",
        "            # Dakshina format: tab-separated without header\n",
        "            with open(path, encoding='utf-8') as f:\n",
        "                for ln in f:\n",
        "                    parts = ln.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        # In Dakshina, the format is target, source (native, latin)\n",
        "                        tgt, src = parts[0], parts[1]\n",
        "                        src_ids = src_vocab.encode(src, add_sos=True, add_eos=True)\n",
        "                        tgt_ids = tgt_vocab.encode(tgt, add_sos=True, add_eos=True)\n",
        "                        self.examples.append((\n",
        "                            torch.tensor(src_ids, dtype=torch.long),\n",
        "                            torch.tensor(tgt_ids, dtype=torch.long)\n",
        "                        ))\n",
        "        else:\n",
        "            # CSV format\n",
        "            df = pd.read_csv(path, header=None)\n",
        "            for _, row in df.iterrows():\n",
        "                src = row[0]\n",
        "                tgt = row[1]\n",
        "                src_ids = src_vocab.encode(src, add_sos=True, add_eos=True)\n",
        "                tgt_ids = tgt_vocab.encode(tgt, add_sos=True, add_eos=True)\n",
        "                self.examples.append((\n",
        "                    torch.tensor(src_ids, dtype=torch.long),\n",
        "                    torch.tensor(tgt_ids, dtype=torch.long)\n",
        "                ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1WmzLqQ4A3b"
      },
      "source": [
        "## 3. Dataset Downloading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xfaTWSyH4A3b"
      },
      "outputs": [],
      "source": [
        "# Dataset download and processing\n",
        "def download_dakshina_dataset(base_dir=None):\n",
        "    \"\"\"Download and extract the Dakshina dataset\"\"\"\n",
        "    if base_dir is None:\n",
        "        base_dir = './dakshina'\n",
        "\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # File paths\n",
        "    tar_path = os.path.join(base_dir, 'dakshina_dataset_v1.0.tar')\n",
        "    extract_path = os.path.join(base_dir, 'dakshina_dataset_v1.0')\n",
        "\n",
        "    # Check if the dataset is already downloaded and extracted\n",
        "    if os.path.exists(extract_path):\n",
        "        print(f\"Dakshina dataset already exists at {extract_path}\")\n",
        "        return extract_path\n",
        "\n",
        "    # Download the dataset\n",
        "    print(\"Downloading Dakshina dataset...\")\n",
        "    url = \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "\n",
        "    # Use subprocess.run for better compatibility across platforms\n",
        "    try:\n",
        "        subprocess.run(['wget', url, '-O', tar_path], check=True)\n",
        "    except:\n",
        "        # Fallback for systems without wget\n",
        "        try:\n",
        "            import requests\n",
        "            print(\"Using requests to download (this may take a while)...\")\n",
        "            response = requests.get(url, stream=True)\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            block_size = 1024  # 1 Kibibyte\n",
        "\n",
        "            with open(tar_path, 'wb') as f:\n",
        "                for i, data in enumerate(response.iter_content(block_size)):\n",
        "                    if i % 1000 == 0:\n",
        "                        progress = i * block_size / total_size * 100 if total_size > 0 else 0\n",
        "                        print(f\"Downloaded {progress:.1f}%\")\n",
        "                    f.write(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading: {e}\")\n",
        "            print(\"Failed to download the dataset. Please download it manually from:\")\n",
        "            print(url)\n",
        "            return None\n",
        "\n",
        "    # Extract the dataset\n",
        "    print(f\"Extracting to {extract_path}...\")\n",
        "    try:\n",
        "        subprocess.run(['tar', '-xf', tar_path, '-C', base_dir], check=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting: {e}\")\n",
        "        print(\"Failed to extract the dataset. Please extract it manually.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Dakshina dataset extracted to {extract_path}\")\n",
        "    return extract_path\n",
        "\n",
        "def collate_fn(batch, src_vocab, tgt_vocab):\n",
        "    \"\"\"Collate function to handle variable-length sequences\"\"\"\n",
        "    srcs, tgts = zip(*batch)\n",
        "    srcs_p = pad_sequence(srcs, batch_first=True, padding_value=src_vocab.pad_idx)\n",
        "    tgts_p = pad_sequence(tgts, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
        "    src_lens = torch.tensor([len(s) for s in srcs], dtype=torch.long)\n",
        "    return srcs_p, src_lens, tgts_p\n",
        "\n",
        "def get_dataloaders(\n",
        "        language='te',\n",
        "        dataset_format='dakshina',\n",
        "        base_path=None,\n",
        "        batch_size=64,\n",
        "        device='cpu',\n",
        "        cache_dir='./cache',\n",
        "        use_cached_vocab=True\n",
        "    ):\n",
        "    \"\"\"Load transliteration datasets for Dakshina\"\"\"\n",
        "    # Create cache directory if it doesn't exist\n",
        "    if use_cached_vocab:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        vocab_cache_path = os.path.join(cache_dir, f\"{language}_{dataset_format}_vocab.pkl\")\n",
        "\n",
        "    # Set up paths based on dataset format\n",
        "    if base_path is None:\n",
        "        dakshina_path = download_dakshina_dataset()\n",
        "        if dakshina_path is None:\n",
        "            # If download failed, try with the default Kaggle path\n",
        "            dakshina_path = '/kaggle/working/dakshina_dataset_v1.0'\n",
        "\n",
        "        base_path = os.path.join(\n",
        "            dakshina_path,\n",
        "            language, 'lexicons'\n",
        "        )\n",
        "\n",
        "    # Try to load cached vocabularies\n",
        "    if use_cached_vocab and os.path.exists(vocab_cache_path):\n",
        "        print(f\"Loading cached vocabularies from {vocab_cache_path}\")\n",
        "        with open(vocab_cache_path, 'rb') as f:\n",
        "            src_vocab, tgt_vocab = pickle.load(f)\n",
        "    else:\n",
        "        # Build vocabularies from data\n",
        "        all_src, all_tgt = [], []\n",
        "\n",
        "        # In Dakshina, files are named like: language.translit.sampled.{split}.tsv\n",
        "        if dataset_format == 'dakshina':\n",
        "            for split in ['train', 'dev']:\n",
        "                path = os.path.join(base_path, f\"{language}.translit.sampled.{split}.tsv\")\n",
        "                with open(path, encoding='utf-8') as f:\n",
        "                    for ln in f:\n",
        "                        parts = ln.strip().split('\\t')\n",
        "                        if len(parts) >= 2:\n",
        "                            # Dakshina format: native script (target), latin script (source)\n",
        "                            all_tgt.append(parts[0])\n",
        "                            all_src.append(parts[1])\n",
        "        else:\n",
        "            # CSV format\n",
        "            train_df = pd.read_csv(os.path.join(base_path, f\"{language}_train.csv\"), header=None)\n",
        "            val_df = pd.read_csv(os.path.join(base_path, f\"{language}_valid.csv\"), header=None)\n",
        "\n",
        "            all_src = train_df[0].tolist() + val_df[0].tolist()\n",
        "            all_tgt = train_df[1].tolist() + val_df[1].tolist()\n",
        "\n",
        "        # Build vocabularies\n",
        "        src_vocab = CharVocab.build_from_texts(all_src)\n",
        "        tgt_vocab = CharVocab.build_from_texts(all_tgt)\n",
        "\n",
        "        # Cache vocabularies\n",
        "        if use_cached_vocab:\n",
        "            with open(vocab_cache_path, 'wb') as f:\n",
        "                pickle.dump((src_vocab, tgt_vocab), f)\n",
        "\n",
        "    # Create data loaders for each split\n",
        "    loaders = {}\n",
        "\n",
        "    if dataset_format == 'dakshina':\n",
        "        splits = {'train': 'train', 'val': 'dev', 'test': 'test'}\n",
        "        for split_name, file_split in splits.items():\n",
        "            path = os.path.join(base_path, f\"{language}.translit.sampled.{file_split}.tsv\")\n",
        "            try:\n",
        "                ds = TransliterationDataset(path, src_vocab, tgt_vocab, format='dakshina')\n",
        "                loaders[split_name] = DataLoader(\n",
        "                    ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=(split_name == 'train'),\n",
        "                    collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab)\n",
        "                )\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Warning: File not found: {path}\")\n",
        "    else:\n",
        "        # CSV format\n",
        "        splits = {\n",
        "            'train': f\"{language}_train.csv\",\n",
        "            'val': f\"{language}_valid.csv\",\n",
        "            'test': f\"{language}_test.csv\"\n",
        "        }\n",
        "\n",
        "        for split_name, file_name in splits.items():\n",
        "            path = os.path.join(base_path, file_name)\n",
        "            try:\n",
        "                ds = TransliterationDataset(path, src_vocab, tgt_vocab, format='csv')\n",
        "                loaders[split_name] = DataLoader(\n",
        "                    ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=(split_name == 'train'),\n",
        "                    collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab)\n",
        "                )\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Warning: File not found: {path}\")\n",
        "\n",
        "    return loaders, src_vocab, tgt_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5J2KH_t4A3c"
      },
      "source": [
        "## 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ra7eHLaj4A3c"
      },
      "outputs": [],
      "source": [
        "# Attention mechanism\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid, dec_hid):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid + dec_hid, dec_hid)\n",
        "        self.v = nn.Linear(dec_hid, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden: [B, H], encoder_outputs: [B, T, H], mask: [B, T]\n",
        "        B, T, H = encoder_outputs.size()\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, T, 1)               # [B, T, H]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [B, T, H]\n",
        "        scores = self.v(energy).squeeze(2)                        # [B, T]\n",
        "        scores = scores.masked_fill(~mask, -1e9)\n",
        "        return torch.softmax(scores, dim=1)                       # [B, T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A0zPLi2n4A3c"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hid_size, layers=1, cell='LSTM', dropout=0.0, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.cell_type = cell\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hid_size\n",
        "\n",
        "        # Output size will be doubled if bidirectional\n",
        "        self.output_size = hid_size * 2 if bidirectional else hid_size\n",
        "\n",
        "        rnn_cls = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}[cell]\n",
        "        self.rnn = rnn_cls(emb_size,\n",
        "                         hid_size,\n",
        "                         num_layers=layers,\n",
        "                         dropout=dropout if layers>1 else 0.0,\n",
        "                         batch_first=True,\n",
        "                         bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, src, lengths):\n",
        "        # src: [B, T], lengths: [B]\n",
        "        embedded = self.embedding(src)  # [B, T, E]\n",
        "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, hidden = self.rnn(packed)\n",
        "        outputs, _ = pad_packed_sequence(packed_out, batch_first=True)  # [B, T, H*dirs]\n",
        "\n",
        "        # If bidirectional, we need to process hidden state properly\n",
        "        if self.bidirectional:\n",
        "            if self.cell_type == 'LSTM':\n",
        "                # For LSTM we have both hidden and cell states\n",
        "                h_n, c_n = hidden\n",
        "                # Combine forward and backward states by averaging\n",
        "                h_n = torch.add(h_n[0:self.layers], h_n[self.layers:]) / 2\n",
        "                c_n = torch.add(c_n[0:self.layers], c_n[self.layers:]) / 2\n",
        "                hidden = (h_n, c_n)\n",
        "            else:\n",
        "                # For GRU/RNN we only have hidden state\n",
        "                hidden = torch.add(hidden[0:self.layers], hidden[self.layers:]) / 2\n",
        "\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OHtuF72I4A3d"
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder with attention support\"\"\"\n",
        "    def __init__(self, vocab_size, emb_size, enc_hid, dec_hid,\n",
        "                 layers=1, cell=\"LSTM\", dropout=0.0, use_attn=True):\n",
        "        super().__init__()\n",
        "        self.use_attn = use_attn\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.cell_type = cell\n",
        "\n",
        "        # ----- dimensions depend on whether we concatenate context -----\n",
        "        if use_attn:\n",
        "            self.attention = BahdanauAttention(enc_hid, dec_hid)\n",
        "            rnn_input_dim = emb_size + enc_hid            # [E ⊕ Henc]\n",
        "            fc_input_dim  = dec_hid + enc_hid + emb_size  # [Hdec ⊕ Henc ⊕ E]\n",
        "        else:\n",
        "            rnn_input_dim = emb_size\n",
        "            fc_input_dim  = dec_hid + emb_size\n",
        "\n",
        "        rnn_cls = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}[cell]\n",
        "        self.rnn = rnn_cls(rnn_input_dim, dec_hid,\n",
        "                           num_layers=layers,\n",
        "                           dropout=dropout if layers > 1 else 0.0,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(fc_input_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs, mask):\n",
        "        \"\"\"Forward pass through the decoder\"\"\"\n",
        "        emb = self.embedding(input_token).unsqueeze(1)     # [B,1,E]\n",
        "        emb = self.dropout(emb)\n",
        "\n",
        "        if self.use_attn:\n",
        "            # ---- additive attention ----\n",
        "            if self.cell_type == 'LSTM':\n",
        "                dec_h = hidden[0][-1]\n",
        "            else:\n",
        "                dec_h = hidden[-1]\n",
        "\n",
        "            attn_w = self.attention(dec_h, encoder_outputs, mask)          # [B,Tenc]\n",
        "            ctx    = torch.bmm(attn_w.unsqueeze(1), encoder_outputs)        # [B,1,Henc]\n",
        "            rnn_in = torch.cat((emb, ctx), dim=2)                           # [B,1,E+Henc]\n",
        "        else:\n",
        "            ctx = None\n",
        "            attn_w = None\n",
        "            rnn_in = emb                                                    # [B,1,E]\n",
        "\n",
        "        out, hidden = self.rnn(rnn_in, hidden)       # [B,1,Hdec]\n",
        "        out = out.squeeze(1)                         # [B,Hdec]\n",
        "        emb = emb.squeeze(1)                         # [B,E]\n",
        "\n",
        "        if self.use_attn:\n",
        "            ctx = ctx.squeeze(1)                     # [B,Henc]\n",
        "            logits = self.fc(torch.cat((out, ctx, emb), dim=1))\n",
        "        else:\n",
        "            logits = self.fc(torch.cat((out, emb), dim=1))\n",
        "\n",
        "        return F.log_softmax(logits, dim=1), hidden, attn_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sxJYUlUd4A3d"
      },
      "outputs": [],
      "source": [
        "# Sequence-to-Sequence model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lens, tgt, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"Forward pass with teacher forcing ratio control\"\"\"\n",
        "        enc_out, hidden = self.encoder(src, src_lens)\n",
        "        mask = (src != self.pad_idx)\n",
        "        B, T = tgt.size()\n",
        "        outputs = torch.zeros(B, T-1, self.decoder.fc.out_features, device=self.device)\n",
        "        input_tok = tgt[:, 0]  # <sos>\n",
        "\n",
        "        for t in range(1, T):\n",
        "            out, hidden, _ = self.decoder(input_tok, hidden, enc_out, mask)\n",
        "            outputs[:, t-1] = out\n",
        "\n",
        "            # Teacher forcing: with probability, use ground truth as next input\n",
        "            # Otherwise use predicted token\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            if teacher_force:\n",
        "                input_tok = tgt[:, t]\n",
        "            else:\n",
        "                input_tok = out.argmax(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def infer_greedy(self, src, src_lens, tgt_vocab, max_len=50):\n",
        "        \"\"\"Greedy inference/generation\"\"\"\n",
        "        enc_out, hidden = self.encoder(src, src_lens)\n",
        "        mask = (src != self.pad_idx)\n",
        "        B = src.size(0)\n",
        "        input_tok = torch.full((B,), tgt_vocab.sos_idx, device=self.device, dtype=torch.long)\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            out, hidden, _ = self.decoder(input_tok, hidden, enc_out, mask)\n",
        "            input_tok = out.argmax(1)\n",
        "            generated.append(input_tok.unsqueeze(1))\n",
        "            if (input_tok == tgt_vocab.eos_idx).all():\n",
        "                break\n",
        "\n",
        "        return torch.cat(generated, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExHP6xTz4A3d"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vH4F8qO04A3d"
      },
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fB04fKDe4A3d"
      },
      "outputs": [],
      "source": [
        "# Evaluation and prediction functions\n",
        "def compute_accuracy(model, loader, tgt_vocab, src_vocab, device):\n",
        "    \"\"\"Compute accuracy on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, src_lens, tgt in loader:\n",
        "            src, src_lens, tgt = (x.to(device) for x in (src, src_lens, tgt))\n",
        "            pred = model.infer_greedy(src, src_lens, tgt_vocab, max_len=tgt.size(1))\n",
        "\n",
        "            # iterate over the batch\n",
        "            for b in range(src.size(0)):\n",
        "                # Convert indices to strings\n",
        "                pred_str = tgt_vocab.decode(pred[b].cpu().tolist())\n",
        "                gold_str = tgt_vocab.decode(tgt[b, 1:].cpu().tolist())  # skip <sos>\n",
        "\n",
        "                # Check if prediction is correct\n",
        "                correct += (pred_str == gold_str)\n",
        "                total += 1\n",
        "\n",
        "    accuracy = correct / total if total else 0.0\n",
        "    return accuracy\n",
        "\n",
        "def predict_examples(model, loaders, src_vocab, tgt_vocab, device, num_examples=5):\n",
        "    \"\"\"Generate predictions for a few examples from the validation set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get some validation examples\n",
        "    val_loader = loaders['val']\n",
        "    val_iter = iter(val_loader)\n",
        "    srcs, src_lens, tgts = next(val_iter)\n",
        "\n",
        "    # Make predictions\n",
        "    srcs, src_lens, tgts = srcs.to(device), src_lens.to(device), tgts.to(device)\n",
        "    outputs = model.infer_greedy(srcs, src_lens, tgt_vocab, max_len=tgts.size(1))\n",
        "\n",
        "    # Print results\n",
        "    print(\"Example predictions:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(min(num_examples, srcs.size(0))):\n",
        "        src_text = src_vocab.decode(srcs[i].cpu().tolist(), strip_specials=True)\n",
        "        tgt_text = tgt_vocab.decode(tgts[i, 1:].cpu().tolist(), strip_specials=True)  # Skip <sos>\n",
        "        pred_text = tgt_vocab.decode(outputs[i].cpu().tolist(), strip_specials=True)\n",
        "\n",
        "        print(f\"Source: {src_text}\")\n",
        "        print(f\"Target: {tgt_text}\")\n",
        "        print(f\"Prediction: {pred_text}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "def visualize_attention(model, src_text, src_vocab, tgt_vocab, device):\n",
        "    \"\"\"Visualize attention weights for a source text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input\n",
        "    src_indices = src_vocab.encode(src_text, add_sos=True, add_eos=True)\n",
        "    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n",
        "    src_lens = torch.tensor([len(src_indices)], dtype=torch.long).to(device)\n",
        "\n",
        "    # Get encoder outputs\n",
        "    enc_outputs, hidden = model.encoder(src_tensor, src_lens)\n",
        "    mask = (src_tensor != src_vocab.pad_idx)\n",
        "\n",
        "    # Initialize decoding\n",
        "    input_tok = torch.tensor([tgt_vocab.sos_idx], device=device)\n",
        "    generated = []\n",
        "    attention_weights = []\n",
        "\n",
        "    # Generate output and collect attention weights\n",
        "    for _ in range(50):  # Max length\n",
        "        out, hidden, attn_w = model.decoder(input_tok, hidden, enc_outputs, mask)\n",
        "        if attn_w is not None:\n",
        "            attention_weights.append(attn_w.squeeze().cpu().detach().numpy())\n",
        "\n",
        "        input_tok = out.argmax(1)\n",
        "        generated.append(input_tok.item())\n",
        "\n",
        "        if input_tok.item() == tgt_vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "    # Convert to text\n",
        "    prediction = tgt_vocab.decode(generated, strip_specials=True)\n",
        "\n",
        "    return prediction, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2qm68yKx4A3e"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(\n",
        "    model,\n",
        "    loaders,\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device,\n",
        "    config,\n",
        "    save_path=None,\n",
        "    log_to_wandb=True\n",
        "):\n",
        "    \"\"\"Train a sequence-to-sequence model\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
        "\n",
        "    # Select optimizer based on config\n",
        "    if config.optimizer.lower() == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    elif config.optimizer.lower() == 'nadam':\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=config.lr)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "    # Track best validation accuracy\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Training batches\n",
        "        for src, src_lens, tgt in loaders['train']:\n",
        "            src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, src_lens, tgt, teacher_forcing_ratio=config.teacher_forcing)\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:,1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss = total_loss / len(loaders['train'])\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for src, src_lens, tgt in loaders['val']:\n",
        "                src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n",
        "                output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)  # No teacher forcing during validation\n",
        "                val_loss += criterion(output.reshape(-1, output.size(-1)),\n",
        "                                    tgt[:,1:].reshape(-1)).item()\n",
        "        val_loss /= len(loaders['val'])\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        train_acc = compute_accuracy(model, loaders['train'], tgt_vocab, src_vocab, device)\n",
        "        val_acc = compute_accuracy(model, loaders['val'], tgt_vocab, src_vocab, device)\n",
        "\n",
        "        # Save model if it's the best so far\n",
        "        if val_acc > best_val_acc and save_path:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Saved new best model with validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        # Log metrics\n",
        "        print(f\"Epoch {epoch}/{config.epochs}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if log_to_wandb:\n",
        "            wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'validation_loss': val_loss,\n",
        "                'train_accuracy': train_acc,\n",
        "                'validation_accuracy': val_acc\n",
        "            })\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    if 'test' in loaders:\n",
        "        test_acc = compute_accuracy(model, loaders['test'], tgt_vocab, src_vocab, device)\n",
        "        print(f\"Final test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "        if log_to_wandb:\n",
        "            wandb.log({'test_accuracy': test_acc})\n",
        "    else:\n",
        "        test_acc = None\n",
        "\n",
        "    return model, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUOPRlaC4A3e"
      },
      "source": [
        "## 6. Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4svHhd9y4A3e"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to train a model using only wandb sweep configuration\"\"\"\n",
        "    # Initialize wandb\n",
        "    run = wandb.init()\n",
        "\n",
        "    # Get the config from wandb\n",
        "    config = run.config\n",
        "\n",
        "    # Map 'cell' to 'cell_type' for compatibility\n",
        "    if 'cell' in config:\n",
        "        config.cell_type = config.cell\n",
        "\n",
        "    # Set default value for dec_layers (same as enc_layers)\n",
        "    config.dec_layers = config.enc_layers if hasattr(config, 'enc_layers') else 1\n",
        "\n",
        "    # Set default value for use_attention (True)\n",
        "    config.use_attention = False\n",
        "\n",
        "    # Set default language and dataset format\n",
        "    config.language = 'te'  # Telugu language\n",
        "    config.dataset_format = 'dakshina'\n",
        "\n",
        "    # Set seeds for reproducibility\n",
        "    seed_everything(config.seed)\n",
        "\n",
        "    # Download and setup Dakshina dataset\n",
        "    dakshina_path = download_dakshina_dataset()\n",
        "    print(f\"Using Dakshina dataset from: {dakshina_path}\")\n",
        "\n",
        "    # Load data\n",
        "    loaders, src_vocab, tgt_vocab = get_dataloaders(\n",
        "        language=config.language,\n",
        "        dataset_format=config.dataset_format,\n",
        "        batch_size=config.batch_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f\"Source vocabulary size: {src_vocab.size}\")\n",
        "    print(f\"Target vocabulary size: {tgt_vocab.size}\")\n",
        "\n",
        "    # Create model\n",
        "    encoder = Encoder(\n",
        "        src_vocab.size, config.emb_size, config.hidden_size,\n",
        "        layers=config.enc_layers, cell=config.cell_type,\n",
        "        dropout=config.dropout, bidirectional=config.bidirectional\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate encoder output dimension (doubled if bidirectional)\n",
        "    enc_out_dim = config.hidden_size * 2 if config.bidirectional else config.hidden_size\n",
        "\n",
        "    decoder = Decoder(\n",
        "        tgt_vocab.size, config.emb_size, enc_out_dim, config.hidden_size,\n",
        "        layers=config.dec_layers, cell=config.cell_type,\n",
        "        dropout=config.dropout, use_attn=config.use_attention\n",
        "    ).to(device)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, src_vocab.pad_idx, device=device).to(device)\n",
        "\n",
        "    # Train model\n",
        "    model, test_acc = train_model(\n",
        "        model=model,\n",
        "        loaders=loaders,\n",
        "        src_vocab=src_vocab,\n",
        "        tgt_vocab=tgt_vocab,\n",
        "        device=device,\n",
        "        config=config,\n",
        "        save_path=\"best_model.pt\",\n",
        "        log_to_wandb=True\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), \"final_model.pt\")\n",
        "\n",
        "    # Generate some predictions\n",
        "    predict_examples(model, loaders, src_vocab, tgt_vocab, device, num_examples=5)\n",
        "\n",
        "    return model, src_vocab, tgt_vocab, loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "RNy9dm8v4A3e"
      },
      "outputs": [],
      "source": [
        "# Run the main function without WandB for simplicity\n",
        "# model, src_vocab, tgt_vocab, loaders = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.login()\n",
        "\n",
        "# Use the exact sweep configuration provided\n",
        "sweep_cfg = {\n",
        "    'method': 'bayes',  # Use Bayesian optimization\n",
        "    'name': 'test_run',\n",
        "    'metric': {'name': 'validation_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        # Model architecture\n",
        "        'emb_size': {'values': [128, 256, 512]},\n",
        "        'hidden_size': {'values': [128, 256, 512, 1024]},\n",
        "        'enc_layers': {'values': [1, 2, 3, 4]},\n",
        "        'cell': {'values': ['RNN', 'GRU', 'LSTM']},\n",
        "        'bidirectional': {'values': [True, False]},  # Bidirectional encode\n",
        "\n",
        "        # Training parameters\n",
        "        'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.5]},\n",
        "        'lr': {'values': [1e-4, 2e-4, 5e-4, 8e-4, 1e-3]},\n",
        "        'batch_size': {'values': [32, 64, 128]},\n",
        "        'epochs': {'values': [10, 15, 20]},\n",
        "        'teacher_forcing': {'values': [0.3, 0.5, 0.7, 1.0]},  # Explicit teacher forcing\n",
        "        'optimizer': {'values': ['Adam', 'NAdam']},  # Added optimizer options\n",
        "        # Reproducibility\n",
        "        'seed': {'values': [42, 43, 44, 45, 46]},  # Different seeds for robustness\n",
        "    }\n",
        "}\n",
        "\n",
        "# Start the sweep\n",
        "sweep_id = wandb.sweep(sweep_cfg, project=\"DA6401_Assignment_3\")\n",
        "wandb.agent(sweep_id, function=main, count=1)  # Run 20 trials (adjust as needed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "O1CKZvATHWYS",
        "outputId": "dc676287-c277-4822-8138-f94a5462d606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}