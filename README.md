# DL_Assignment_3 â€“ Seq2Seq Transliteration with and without Attention

This repository contains the implementation of a character-level sequence-to-sequence (Seq2Seq) model for transliteration using the [Dakshina dataset](https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar). It compares models trained **with attention** and **without attention** using PyTorch.

---

## ğŸ“ Repository Structure

```
DL_Assignment_3/
â”‚
â”œâ”€â”€ WithAttention/
â”‚   â”œâ”€â”€ assignment_3_with_attention.py            # Main script (with attention)
â”‚   â”œâ”€â”€ Assignment_3_with_attention.ipynb         # Notebook version
â”‚   â”œâ”€â”€ readme.md                                 # Specific README for attention version
â”‚   â”œâ”€â”€ withAttention_correct_predictions.txt     # Sample correct outputs
â”‚   â””â”€â”€ withoutAttention_incorrect_predictions.txt# Sample errors for comparison
â”‚
â”œâ”€â”€ WithoutAttention/
â”‚   â”œâ”€â”€ assignment_3_without_attention.py         # Main script (no attention)
â”‚   â”œâ”€â”€ Assignment_3_without_attention.ipynb      # Notebook version
â”‚   â”œâ”€â”€ readme.md                                 # Specific README for non-attention version
â”‚   â”œâ”€â”€ withoutAttention_correct_predictions.txt  # Correct output samples
â”‚   â””â”€â”€ withoutAttention_incorrect_predictions.txt# Incorrect predictions
â”‚
â””â”€â”€ README.md                                     # Root README (this file)
```

---

## ğŸ§  Project Overview

The task is to transliterate from Latin-script input to the native script of Indian languages using Seq2Seq models. Two architectures are implemented:

- `WithAttention`: Uses Bahdanau Attention for improved sequence modeling.
- `WithoutAttention`: Basic encoder-decoder model without attention mechanism.

---

## ğŸ”§ Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/Dhanushsirigineedi/DL_Assignment_3.git
   cd DL_Assignment_3
   ```

2. Install dependencies:
   ```bash
   pip install torch pandas numpy matplotlib wandb
   ```

3. (Optional) Login to Weights & Biases for tracking:
   ```bash
   wandb login
   ```

---

## ğŸš€ How to Run

Navigate to one of the folders (`WithAttention` or `WithoutAttention`) and run:

```bash
python assignment_3_with_attention.py
```

or

```bash
python assignment_3_without_attention.py
```

You can also open the `.ipynb` notebooks in Colab or Jupyter to visualize training step-by-step.

---

## ğŸ“Š Output

- Model predictions are evaluated and written to:
  - `withoutAttention_correct_predictions.txt`
  - `withoutAttention_incorrect_predictions.txt`
- Accuracy and loss are logged for training, validation, and optionally test sets.
- Visual and quantitative performance comparison between attention and non-attention models.

---

## ğŸ“‚ Dakshina Dataset

The script automatically downloads and processes the dataset. If needed, you can download it manually from [Google Research](https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar).

---

## ğŸ“ˆ Experiment Tracking

This project supports [Weights & Biases](https://wandb.ai) for experiment tracking. You can log training/validation metrics and visualize results online.

---

## âœï¸ Authors

- Dhanush Sirigineedi

---
